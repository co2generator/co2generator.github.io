[{"content":"I\u0026rsquo;d like to order you a song. Let me whisper to you in this way many things that cannot be said\u0026hellip;\nLife will hand you lemons, often when you least expect it. Life is not fair. However, we should also believe that although good things are often hard-won, they are on the way! I am going to share with you a short poem, which is named New York is 3 hours ahead of California,\nNew York is 3 hours ahead of California,\nbut it does not make California slow.\nSomeone graduated at the age of 22,\nbut waited 5 years before securing a good job!\nSomeone became a CEO at 25,\nand died at 50\nWhile another became a CEO at 50,\nand lived to 90 years.\nSomeone is still single,\nwhile someone else got married.\nObama retires at 55,\nbut Trump starts at 70.\nAbsolutely everyone in this world works based on their Time Zone.\nPeople around you might seem to go ahead of you,\nsome might seem to be behind you.\nBut everyone is running their own RACE, in their own TIME.\nDon’t envy them or mock them.\nThey are in their TIME ZONE, and you are in yours!\nLife is about waiting for the right moment to act.\nSo, RELAX.\nYou’re not LATE.\nYou’re not EARLY.\nYou are very much ON TIME, and in your TIME ZONE Destiny set up for you.\nFinally, Yesterday is history, tomorrow is mystery. Today is a gift. That is why it is called the present.\nMay you a bright future.\n","permalink":"http://co2generator.github.io/posts/mem.2022.0814/","summary":"I\u0026rsquo;d like to order you a song. Let me whisper to you in this way many things that cannot be said\u0026hellip;\nLife will hand you lemons, often when you least expect it. Life is not fair. However, we should also believe that although good things are often hard-won, they are on the way! I am going to share with you a short poem, which is named New York is 3 hours ahead of California,","title":"To an Old Friend"},{"content":"This week, I read the paper Lozano et al.(2016). The following are the reading notes.\n1 Background The Elementary Shortest Path Problem with Resource Constraints (ESPPRC) is NP-hard in the strong sense and often arises as the backbone of the branch-and-price procedure. Ac- celerating the solution of the ESPPRC is a critical aspect for improving the performance of column-generation based algorithms. The ESPPRC has been widely studied,\nDesrochers et al. (1992) solved a relaxed version that allows cycles with a dynamic pro- gramming.\nFeillet et al. (2004) proposed the first exact approach for the ESPPRC.\nRighini and Salani (2008) proposed a bidirectional labeling algorithm for the ESPPRC that\nrelies on a state-space relaxation.\nc proposed an novel exact method which performs well when compared against SOTA algorithms for the ESPPRC.\nIn this report, the motivation, overview and details of the algorithm (Pulse Algorithm) proposed by Lozano et al. (2016) are reported.\n2 Motivation \u0026amp; Overview Definition 1. Pulse Propagation refers to the recursive exploration of partial paths that are extended until they reach the end node or are discarded by a pruning strategy.\nThe pulse algorithm relies on implicit enumeration with a novel bounding scheme that dra- matically narrows the search space. Specifically, for ESPPRC, the pulse algorithm comprises two stages:\na bounding stage that finds lower bounds on the cost given an amount of resource consumed.\na recursive exploration stage that finds an optimal solution based on an implicit enumeration\nof the solution space.\nThe exploration is triggered by sending a pulse from the start node $v_s \\in \\mathscr{N}$. The pulse tries to propagate throughout the outgoing arcs of each visited node, storing at each node the partial path $\\mathscr{P}$, the cumulative reduced cost $r(\\mathscr{P})$, the cumulative capacity consumption $q(\\mathscr{P})$, and the cumulative time consumption $t(\\mathscr{P})$.\nAt each node different pruning strategies try to stop the pulse propagation, aggressively pruning the search space. Ever pulse that reaches the final node $v_e \\in \\mathscr{N}$ contains all of the information of a feasible paths $\\mathscr{P}$ from $v_s$ to $v_e$. The pseudo-code is shown below,\nThe pseudo-code of pulse procedure is shown below,\nEvery time the pulse procedure is invoked on the final node $v_e$, the information for the best-known path $\\mathscr{P}^*$ is updated, the pulse stops its propagation, and the algorithm backtracks to propagate other pulses recursively.\n\\qquad Clearly, line 2 and line 3 play the role of pruning. And it is worth highlighting that every time that a pulse is pruned, a whole entire region of the solution space is discarded. Thus, the algorithm\u0026rsquo;s performance is strongly linked to the strength of the pruning strategies and their ability to prune pules at early stages of the exploration.\n3 Pruning Strategies As shown in Algorithm 3, there are three pruning strategies.\nUse structural constraints to prune infeasible solutions Use primal and lower bounds to prune suboptimal solutions Use a look-back mechanism to discard suboptimal partial paths The first pruning strategy is straightforward (check time window, capacity and cycle constraints). The second is critically important. The pseudo-code is shown below,\nThe third one is also important. The behind idea is very simple. One of the main drawbacks of depth-first search is that poor decisions made at early stages of the exploration can lead to the exploration of unpromising regions of the search space.\nConsider a partial path $\\mathscr{P}_{si}$ from $v_s$ to $v_i$ that is extended to node $v_k$ and then reaches node $v_j$ as follow,\nLet $\\mathscr{P}^\\prime_{sj} = \\mathscr{P}{si} \\cup \\left\\lbrace v_j\\right\\rbrace$, then clearly it can be proved that $\\mathscr{P}{sj}$ can be dominated by $\\mathscr{P}^\\prime_{sj}$.\n4 Conclusion The pulse algorithm is very similar to the traditional labeling algorithm. The difference is the pulse algorithm explores the graph in a depth-first manner, while the labeling algorithm follow a lexicographic breadth-first search. The pulse algorithm also can be seen as a branch and bound where each node corresponds to an elementary partial path. Another big difference is the pulse algorithm doesn\u0026rsquo;t need to handle a long list ordered labels. And it heavily relies on additional pruning strategies. Reference [1] Desrochers, M., J. Desrosiers, and M. Solomon (1992). A new optimization algorithm for the vehicle routing problem with time windows. Operations research 40(2), 342–354. [2] Feillet, D., P. Dejax, M. Gendreau, and C. Gueguen (2004). An exact algorithm for the elementary shortest path problem with resource constraints: Application to some vehicle routing problems. Networks: An International Journal 44(3), 216–229. [3] Lozano, L., D. Duque, and A. L. Medaglia (2016). An exact algorithm for the elementary shortest path problem with resource constraints. Transportation Science 50(1), 348–357. [4] Righini, G. and M. Salani (2008). New dynamic programming algorithms for the resource con- strained elementary shortest path problem. Networks: An International Journal 51(3), 155– 170. ","permalink":"http://co2generator.github.io/posts/trsc.2014.0582/","summary":"This week, I read the paper Lozano et al.(2016). The following are the reading notes.\n1 Background The Elementary Shortest Path Problem with Resource Constraints (ESPPRC) is NP-hard in the strong sense and often arises as the backbone of the branch-and-price procedure. Ac- celerating the solution of the ESPPRC is a critical aspect for improving the performance of column-generation based algorithms. The ESPPRC has been widely studied,\nDesrochers et al. (1992) solved a relaxed version that allows cycles with a dynamic pro- gramming.","title":"PR02-An Exact Algorithm for the Elmentary Shortest Path Problem with Resource Constraints"},{"content":"This week, I read the paper Delage et al. (2010). The following are the reading notes.\n1 Background $$ \\mathop{\\text{minimize}}\\limits_{\\pmb{x} \\in \\mathscr{X}} \\ h(\\pmb{x}, \\pmb{\\xi}) \\tag{1} $$ in which, $\\mathscr{X}$ is a convex set of feasible solutions and $h(\\pmb{x}, \\pmb{\\xi})$ is a convex cost function in $\\pmb{x}$ that depends on some vector of parameters $\\pmb{\\xi}$. In practice, it is often the case that at the time of optimization, the parameters have not yet been fully resolved.\nIf one choose to represent the uncertainty of $\\pmb{\\xi}$ through a distribution $F$, one can instead resort to minimizing the expected cost. This leads to solving a stochastic program, $$ (\\text{SP}) \\quad \\mathop{\\text{minimize}} \\limits_{\\pmb{x} \\in \\mathscr{X}} \\ \\mathbb{E}[h(\\pmb{x}, \\pmb{\\xi})] \\tag{2} $$\nin which the expectation is taken with respect to the random parameters $\\pmb{\\xi} \\in \\mathbb{R}^m$, given that it follows the probability distribution $F$. But unfortunately, although the SP is a convex optimization problem, to solve it one must often resort to Monte Carlo approximation, which can be computationally challenging. A more challenging difficulty is the need to commit to a distribution $F$ given only limited information about the stochastic parameters.\nTo address these issues, a robust formulation for SP was proposed by Scarf(1958). In this model, the true distribution $F$ is assumed to be included in a set of probability distributions $\\mathscr{D}$. And the objective function is reformulated with respect to he worst case expected cost over the choice of a distribution in this set. This leads to solving the distributionally robust stochastic program, $$ (\\text{DRSP}) \\quad \\mathop{\\text{minimize}} \\limits_{\\pmb{x} \\in \\mathscr{X}} \\left(\\max_{F \\in \\mathscr{D}} \\mathbb{E}_{F} [h(\\pmb{x}, \\pmb{\\xi})] \\right) \\tag{3} $$\nin which $\\mathbb{E}_F[\\cdot]$ is the expectation taken with respect to the random vector $\\pmb{\\xi}$, given that it follows the probability distribution $F$.\n2 Motivation Since the information about the distribution $F$ is limited, it might instead be safer to rely on estimated of the mean $\\pmb{\\mu}_0$ and covariance matrix $\\pmb{\\Sigma}_0$ of the random vector.\nHowever, it is believed that it is rarely the case that one is entirely confident in these estimates. Hence, the authors proposed two hyper-parameters $\\gamma_1$ and $\\gamma_2$ to quantifying one\u0026rsquo;s confidence in $\\pmb{\\mu}_0$ and $\\pmb{\\Sigma}_0$.\nFor the mean of $\\pmb{\\xi}$, they assumed that it lies in an ellipsoid of size $\\gamma_1$ centered at the estimate $\\pmb{\\mu}_0$, $$ (\\mathbb{E}[\\pmb{\\xi}] - \\pmb{\\mu}_0)^\\top \\pmb{\\Sigma}_0^{-1} (\\mathbb{E}[\\pmb{\\xi}] - \\pmb{\\mu}_0) \\leq \\gamma_1 \\tag{4} $$\nFor the covariance of $\\pmb{\\xi}$, the so-called second-moment is used, $$ \\mathbb{E}[(\\pmb{\\xi} - \\pmb{\\mu}_0) (\\pmb{\\xi} - \\pmb{\\mu}_0)^\\top] \\preceq \\gamma_2 \\pmb{\\Sigma}_0 \\tag{5} $$\nTotally, the distributional set is formulated as, $$ \\mathscr{D}_1 (\\mathscr{S}, \\pmb{\\mu}_0, \\pmb{\\Sigma}_0, \\gamma_1, \\gamma_2) = \\left \\lbrace \\begin{aligned} F \\in \\mathscr{M} \\left| \\begin{aligned} \\ \u0026amp; \\mathbb{P}(\\pmb{\\xi} \\in \\mathscr{S}) = 1 \\newline \u0026amp; (\\mathbb{E}[\\pmb{\\xi}] - \\pmb{\\mu}_0)^\\top \\pmb{\\Sigma}_0^{-1} (\\mathbb{E}[\\pmb{\\xi}] - \\pmb{\\mu}_0) \\leq \\gamma_1\\newline \u0026amp; \\mathbb{E}[(\\pmb{\\xi} - \\pmb{\\mu}_0) (\\pmb{\\xi} - \\pmb{\\mu}_0)^\\top] \\preceq \\gamma_2 \\pmb{\\Sigma}_0 \\end{aligned} \\right. \\end{aligned}\\right \\rbrace \\tag{6} $$ in which $\\mathscr{M}$ is the set of probability measures, $\\mathscr{S}$ is any closed convex set known to contain the support of $F$.\n3 Distributionally Robust Stochastic Optimization Consider the following DRSP model under the distributional set $\\mathscr{D}_1$,\n$$ (\\text{DRSP}) \\quad \\mathop{\\text{minimize}} \\limits_{\\pmb{x} \\in \\mathscr{X}} \\left( \\max_{F \\in \\mathscr{D}1} \\mathbb{E}{F} [h(\\pmb{x}, \\pmb{\\xi})] \\right) \\tag{7} $$ The authors analyzed the computation complexity of the inner moment problem and addressed the tractable solution method of the whole problem.\n3.1 Complexity of the Inner Moment Problem Let $\\Psi(\\pmb{x}, \\gamma_1, \\gamma_2)$ be the optimal value of the moment problem, $$ \\mathop{\\text{maximize}} \\limits_{F \\in \\mathscr{D}1} \\quad \\mathbb{E}F[h(\\pmb{x}, \\pmb{\\xi})] \\tag{8} $$ It also can be described as the conic linear problem, $$ \\begin{aligned} \\mathop{\\text{maximize}}\\limits{F} \\quad \u0026amp; \\int{\\mathscr{S}} h(\\pmb{x}, \\pmb{\\xi}) dF(\\pmb{\\xi}) \\newline \\mathop{\\text{subject to}} \\quad \u0026amp; \\left\\lbrace \\begin{aligned} \u0026amp; \\int_{\\mathscr{S}} dF(\\pmb{\\xi}) = 1, \\newline \u0026amp; \\int_{\\mathscr{S}} (\\pmb{\\xi} - \\pmb{\\mu}_0) (\\pmb{\\xi} - \\pmb{\\mu}_0)^\\top dF(\\pmb{\\xi}) \\preceq \\gamma_2 \\pmb{\\Sigma}0, \\newline \u0026amp; \\int{\\mathscr{S}} \\begin{bmatrix} \\pmb{\\Sigma}_0 \u0026amp; (\\pmb{\\xi} - \\pmb{\\mu}_0) \\newline (\\pmb{\\xi} - \\pmb{\\mu}_0)^\\top \u0026amp; \\gamma_1 \\end{bmatrix} dF(\\pmb{\\xi}) \\succeq 0 \\newline \u0026amp; F \\in \\mathscr{M} \\end{aligned} \\right. \\end{aligned} \\tag{9} $$\nTheorem 1. For a fixed $\\pmb{x} \\in \\mathbb{R}^n$, suppose that $\\gamma_1 \\geq 0, \\gamma_2 \\geq 1, \\pmb{\\Sigma}_0 \\succ 0$, and that $h(\\pmb{x}, \\pmb{\\xi})$ is F-integrable for all $F \\in \\mathscr{D}_1$. Then $\\Psi(\\pmb{x}, \\gamma_1, \\gamma_2)$ must be equal to the optimal value of the problem,\n$$ \\begin{aligned} \\mathop{\\text{minimize}}\\limits_{\\mathbf{Q}, \\mathbf{q}, r, t} \\quad \u0026amp; r + t \\label{inner-obj} \\newline \\text{subject to} \\quad \u0026amp; \\left\\lbrace \\begin{aligned} \u0026amp; r \\geq h(\\pmb{x}, \\pmb{\\xi}) - \\pmb{\\xi}^\\top \\mathbf{Q} \\pmb{\\xi} - \\pmb{\\xi}^\\top \\mathbf{q}, \\qquad \\forall \\pmb{\\xi} \\in \\mathscr{S} \\newline \u0026amp; t \\geq (\\gamma_2 \\pmb{\\Sigma}_0 + \\pmb{\\mu}_0 \\pmb{\\mu}_0^\\top ) \\bullet \\mathbf{Q} + \\pmb{\\mu}_0^\\top \\mathbf{q} + \\sqrt{\\gamma_1} | \\pmb{\\Sigma}_0^{1 / 2} (\\mathbf{q} + 2 \\mathbf{Q}\\pmb{\\mu}_0) |, \\newline \u0026amp; \\mathbf{Q} \\succeq 0 \\end{aligned} \\right. \\label{inner-cons} \\end{aligned} \\tag{10} $$\nin which $(\\pmb{A} \\bullet \\pmb{B}) $ refers to the Frobenius inner product between matrices, $\\mathbf{Q} \\in \\mathbb{R}^{m \\times m}$ is a symmetric matrix, the vector $\\mathbf{q} \\in \\mathbb{R}^m$, and $r, t \\in \\mathbb{R}$.\nLemma 1. (Grotschel et al. (1981)) Consider a convex optimization problem of the form, $$ \\mathop{\\text{minimize}} \\limits_{\\pmb{z} \\in \\mathscr{Z}} \\ \\pmb{c}^\\top \\pmb{z} $$ with linear objective and convex feasible set $\\mathscr{Z}$. Given that the set of optimal solutions is nonempty, the problem can be solved to any precision $\\epsilon$ in time polynomial in $\\log(1 / \\epsilon)$ and in the size of the problem by using ellipsoid method is and only if $\\mathscr{Z}$ satisfies the following two conditions:\nfor any $\\bar{\\pmb{z}}$, it can be verified whether $\\bar{\\pmb{z}} \\in \\mathscr{Z}$ or not in time polynomial in the dimension of $\\pmb{z}$ for any infeasible $\\bar{\\pmb{z}}$, a hyperplane that separates $\\bar{\\pmb{z}}$ from the feasible set $\\mathscr{Z}$ can be generated in time polynomial in the dimension of $\\pmb{z}$. Assumption 1. The support set $\\mathscr{S} \\subset \\mathbb{R}^m$ is convex and compact, and it is equipped with an oracle that can for any $\\pmb{\\xi} \\in \\mathbb{R}^m$ either confirm that $\\pmb{\\xi} \\in \\mathscr{S}$ or provide a hyperplane that separates $\\pmb{\\xi}$ from $\\mathscr{S}$ in time polynomial in m.\nAssumption 2. The function $h(\\pmb{x}, \\pmb{\\xi})$ has the form $h(\\pmb{x}, \\pmb{\\xi}) = \\max_{k \\in \\left\\lbrace 1, \u0026hellip;, K\\right\\rbrace} h_k(\\pmb{x}, \\pmb{\\xi})$ such that for each $k, h_k(\\pmb{x}, \\pmb{\\xi})$ is concave in $\\pmb{\\xi}$. In addition, given a pair $(\\pmb{x}, \\pmb{\\xi})$, it is assumed that one can in polynomial time:\nevaluate the value of $h_k(\\pmb{x}, \\pmb{\\xi}) $ find a supergradient of $h_k(\\pmb{x}, \\pmb{\\xi})$ in $\\pmb{\\xi}$ Furthermore, for any $\\pmb{x} \\in \\mathbb{R}^n, \\pmb{q} \\in \\mathbb{R}^m$, and ay positive semidefinite $\\mathbf{Q} \\in \\mathbb{R}^{m \\times m}$, the set $\\left\\lbrace y \\in \\mathbb{R} | \\exists \\pmb{\\xi} \\in \\mathscr{S}, y \\leq h(\\pmb{x}, \\pmb{\\xi}) - \\pmb{q}^\\top \\pmb{\\xi} - \\pmb{\\xi}^\\top \\mathbf{Q} \\pmb{\\xi} \\right\\rbrace$ is closed.\nTheorem 2. Given that $\\mathscr{S}$ satisfies Assumption 1 and that $h(\\pmb{x}, \\pmb{\\xi})$ satisfies Assumption 2 and satisfies the condition of Lemma 1, then problem (10) is a convex optimization problem whose optimal value is finite and equal to $\\Psi(\\pmb{x}, \\gamma_1, \\gamma_2)$. Moreover, problem (10) can be solved to any precision $\\epsilon$ in time polynomial in $\\log (1 / \\epsilon)$ and the size of the problem.\n3.2 Complexity of the DRSP $$ (\\text{DRSP})\\mathop{\\text{minimize}} \\limits_{\\pmb{x} \\in \\mathscr{X}} \\left(\\max_{F \\in \\mathscr{D}1} \\mathbb{E}{F} [h(\\pmb{x}, \\pmb{\\xi})] \\right) \\tag{11} $$\nAssumption 3. The $\\mathscr{X} \\subset \\mathbb{R}^n$ is convex and compact, and it is equipped with an oracle that can for any $\\pmb{x} \\in \\mathbb{R}^n$ either confirm that $\\pmb{x} \\in \\mathscr{X}$ or provide a hyperplane that separates $\\pmb{x}$ from $\\mathscr{X}$ in time polynomial n.\nAssumption 4. The function $h(\\pmb{x}, \\pmb{\\xi})$ is convex in $\\pmb{x}$. In addition, it is assumed that one can find in polynomial time a subgradient of $h(\\pmb{x}, \\pmb{\\xi})$ in $\\pmb{x}$.\nTheorem 3. Given that Assumption 1, 2, 3 and 4 hold, then the DRSP model presented in problem (11) can be solved to any precision $\\epsilon$ in time polynomial in $\\log(1 / \\epsilon)$ and the sizes of $\\pmb{x}$ and $\\pmb{\\xi}$.\nThe proof idea is to transform the DRSP to the following convex optimization problem, $$ \\begin{aligned} \\mathop{\\text{minimize}}\\limits_{\\mathbf{Q}, \\mathbf{q}, r, t} \\quad \u0026amp; r + t \\newline \\text{subject to} \\quad \u0026amp; \\left\\lbrace \\begin{aligned} \u0026amp; r \\geq h_k(\\pmb{x}, \\pmb{\\xi}) - \\pmb{\\xi}^\\top \\mathbf{Q} \\pmb{\\xi} - \\pmb{\\xi}^\\top \\mathbf{q},\\quad \\forall \\pmb{\\xi} \\in \\mathscr{S}, k \\in \\left\\lbrace 1, \u0026hellip;, K\\right\\rbrace, \\newline \u0026amp; t \\geq (\\gamma_2 \\pmb{\\Sigma}_0 + \\pmb{\\mu}_0 \\pmb{\\mu}_0^\\top ) \\bullet \\mathbf{Q} + \\pmb{\\mu}_0^\\top \\mathbf{q} + \\sqrt{\\gamma_1} | \\pmb{\\Sigma}_0^{1 / 2} (\\mathbf{q} + 2 \\mathbf{Q}\\pmb{\\mu}_0) |, \\newline \u0026amp; \\mathbf{Q} \\succeq 0 \\newline \u0026amp; \\pmb{x} \\in \\mathscr{X} \\end{aligned} \\right. \\end{aligned} $$ which can be solved to any precision $\\epsilon$ in time polynomial in $\\log (1 / \\epsilon)$ with ellipsoid method if these assumptions hold.\n4 Conclusion This paper proposed the moment-based distributional set. And to my best knowledge, it is the first time that such robust stochastic optimization approach is named distributionally robust optimization (DRO). To some extent, it opens the era of DRO. One important thing I get from this paper is that, not all the convex problems are easy to solve. Some problem has no polynomial time algorithm. The problem type discussed in this paper is too general for me. The next week I am going to read a more practical paper. Reference Delage, E. and Y. Ye (2010). Distributionally robust optimization under moment uncertainty with application to data-driven problems. Operations Research 58(3), 595–612. Grotschel, M., L. Lov ́asz, and A. Schrijver (1981). The ellipsoid method and its consequences in combinatorial optimization. Combinatorica 1(2), 169–197. Scarf, H. (1958). A min-max solution of an inventory problem. Studies in the mathematical theory of inventory and production. ","permalink":"http://co2generator.github.io/posts/opre.1090.0741/","summary":"This week, I read the paper Delage et al. (2010). The following are the reading notes.\n1 Background $$ \\mathop{\\text{minimize}}\\limits_{\\pmb{x} \\in \\mathscr{X}} \\ h(\\pmb{x}, \\pmb{\\xi}) \\tag{1} $$ in which, $\\mathscr{X}$ is a convex set of feasible solutions and $h(\\pmb{x}, \\pmb{\\xi})$ is a convex cost function in $\\pmb{x}$ that depends on some vector of parameters $\\pmb{\\xi}$. In practice, it is often the case that at the time of optimization, the parameters have not yet been fully resolved.","title":"PR01-Distributionally Robust Optimization Under Moment Uncertainty"}]